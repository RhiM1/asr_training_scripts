{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-10-27 18:39:15 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n",
      "[NeMo W 2022-10-27 18:39:17 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-10-27 18:39:17 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-10-27 18:39:17 nemo_logging:349] /exp/exp1/acp21rjf/lhotse/lhotse/lazy.py:388: UserWarning: A lambda was passed to LazyMapper: it may prevent you from forking this process. If you experience issues with num_workers > 0 in torch.utils.data.DataLoader, try passing a regular function instead.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'tedlium'\n",
      "/exp/exp1/acp21rjf/deliberation/speachy/tedlium\n"
     ]
    }
   ],
   "source": [
    "import tools\n",
    "corpus = tools.load_corpus()\n",
    "from importlib import reload as rl\n",
    "import non_iid_dataloader as niiddl, lhotse\n",
    "partition = niiddl.prepare_partition(corpus['train'])\n",
    "from tqdm import tqdm\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer, Transformer\n",
    "import torch\n",
    "import x_transformers, torch\n",
    "tk = tools.load_tokenizer()\n",
    "from omegaconf.omegaconf import OmegaConf\n",
    "%cd tedlium\n",
    "import lm_utils\n",
    "tokenizer = tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "meetings = niiddl.prepare_partition(corpus['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lhotse\n",
    "import non_iid_dataloader as niiddl\n",
    "rl(niiddl),rl(lhotse)\n",
    "from lhotse.dataset.collation import collate_audio\n",
    "from lhotse.dataset.cut_transforms import plain_concat, individual_speaker_concat\n",
    "niiddl.plain_concat = plain_concat\n",
    "niiddl.individual_speaker_concat = individual_speaker_concat\n",
    "\n",
    "samples = niiddl.prepare_samples(meetings, max_allowed_utterance_gap=5.0, max_duration=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "rl(tools)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_utils\n",
    "class argsclass:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl(lm_utils)\n",
    "model = lm_utils.load_model(lm_utils.load_config('./lm/decoder_test.yaml'), tools.load_tokenizer(), max_len=1862)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerWrapper(\n",
       "  (token_emb): TokenEmbedding(\n",
       "    (emb): Embedding(128, 256)\n",
       "  )\n",
       "  (post_emb_norm): Identity()\n",
       "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (project_emb): Identity()\n",
       "  (attn_layers): Decoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (1): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (2): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (3): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (4): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (5): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (6): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (7): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (8): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (9): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (10): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (11): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (12): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (13): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (14): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (15): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (16): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (17): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (18): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (19): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (20): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (21): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (22): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (23): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (24): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (25): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (26): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (27): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (28): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (29): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (30): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): Attention(\n",
       "          (to_q): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_k): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (to_v): Linear(in_features=256, out_features=512, bias=False)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (to_out): Linear(in_features=512, out_features=256, bias=False)\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "      (31): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): None\n",
       "          (2): None\n",
       "        )\n",
       "        (1): FeedForward(\n",
       "          (net): Sequential(\n",
       "            (0): Sequential(\n",
       "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (1): GELU()\n",
       "            )\n",
       "            (1): Identity()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Residual()\n",
       "      )\n",
       "    )\n",
       "    (rotary_pos_emb): RotaryEmbedding()\n",
       "  )\n",
       "  (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  (to_logits): Linear(in_features=256, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_46_id_32.pt\tcheckpoint_50_id_80.pt\tcheckpoint_52_id_26.pt\n",
      "checkpoint_47_id_60.pt\tcheckpoint_51_id_44.pt\n"
     ]
    }
   ],
   "source": [
    "!ls checkpoints_LM3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints_15_mediumplus_tlm3000vocab/checkpoint_7_id_13.pt\n",
      "odict_keys(['token_emb.emb.weight', 'attn_layers.layers.0.0.0.weight', 'attn_layers.layers.0.0.0.bias', 'attn_layers.layers.0.1.to_q.weight', 'attn_layers.layers.0.1.to_k.weight', 'attn_layers.layers.0.1.to_v.weight', 'attn_layers.layers.0.1.to_out.weight', 'attn_layers.layers.1.0.0.weight', 'attn_layers.layers.1.0.0.bias', 'attn_layers.layers.1.1.net.0.0.weight', 'attn_layers.layers.1.1.net.0.0.bias', 'attn_layers.layers.1.1.net.3.weight', 'attn_layers.layers.1.1.net.3.bias', 'attn_layers.layers.2.0.0.weight', 'attn_layers.layers.2.0.0.bias', 'attn_layers.layers.2.1.to_q.weight', 'attn_layers.layers.2.1.to_k.weight', 'attn_layers.layers.2.1.to_v.weight', 'attn_layers.layers.2.1.to_out.weight', 'attn_layers.layers.3.0.0.weight', 'attn_layers.layers.3.0.0.bias', 'attn_layers.layers.3.1.net.0.0.weight', 'attn_layers.layers.3.1.net.0.0.bias', 'attn_layers.layers.3.1.net.3.weight', 'attn_layers.layers.3.1.net.3.bias', 'attn_layers.layers.4.0.0.weight', 'attn_layers.layers.4.0.0.bias', 'attn_layers.layers.4.1.to_q.weight', 'attn_layers.layers.4.1.to_k.weight', 'attn_layers.layers.4.1.to_v.weight', 'attn_layers.layers.4.1.to_out.weight', 'attn_layers.layers.5.0.0.weight', 'attn_layers.layers.5.0.0.bias', 'attn_layers.layers.5.1.net.0.0.weight', 'attn_layers.layers.5.1.net.0.0.bias', 'attn_layers.layers.5.1.net.3.weight', 'attn_layers.layers.5.1.net.3.bias', 'attn_layers.layers.6.0.0.weight', 'attn_layers.layers.6.0.0.bias', 'attn_layers.layers.6.1.to_q.weight', 'attn_layers.layers.6.1.to_k.weight', 'attn_layers.layers.6.1.to_v.weight', 'attn_layers.layers.6.1.to_out.weight', 'attn_layers.layers.7.0.0.weight', 'attn_layers.layers.7.0.0.bias', 'attn_layers.layers.7.1.net.0.0.weight', 'attn_layers.layers.7.1.net.0.0.bias', 'attn_layers.layers.7.1.net.3.weight', 'attn_layers.layers.7.1.net.3.bias', 'attn_layers.layers.8.0.0.weight', 'attn_layers.layers.8.0.0.bias', 'attn_layers.layers.8.1.to_q.weight', 'attn_layers.layers.8.1.to_k.weight', 'attn_layers.layers.8.1.to_v.weight', 'attn_layers.layers.8.1.to_out.weight', 'attn_layers.layers.9.0.0.weight', 'attn_layers.layers.9.0.0.bias', 'attn_layers.layers.9.1.net.0.0.weight', 'attn_layers.layers.9.1.net.0.0.bias', 'attn_layers.layers.9.1.net.3.weight', 'attn_layers.layers.9.1.net.3.bias', 'attn_layers.layers.10.0.0.weight', 'attn_layers.layers.10.0.0.bias', 'attn_layers.layers.10.1.to_q.weight', 'attn_layers.layers.10.1.to_k.weight', 'attn_layers.layers.10.1.to_v.weight', 'attn_layers.layers.10.1.to_out.weight', 'attn_layers.layers.11.0.0.weight', 'attn_layers.layers.11.0.0.bias', 'attn_layers.layers.11.1.net.0.0.weight', 'attn_layers.layers.11.1.net.0.0.bias', 'attn_layers.layers.11.1.net.3.weight', 'attn_layers.layers.11.1.net.3.bias', 'attn_layers.layers.12.0.0.weight', 'attn_layers.layers.12.0.0.bias', 'attn_layers.layers.12.1.to_q.weight', 'attn_layers.layers.12.1.to_k.weight', 'attn_layers.layers.12.1.to_v.weight', 'attn_layers.layers.12.1.to_out.weight', 'attn_layers.layers.13.0.0.weight', 'attn_layers.layers.13.0.0.bias', 'attn_layers.layers.13.1.net.0.0.weight', 'attn_layers.layers.13.1.net.0.0.bias', 'attn_layers.layers.13.1.net.3.weight', 'attn_layers.layers.13.1.net.3.bias', 'attn_layers.layers.14.0.0.weight', 'attn_layers.layers.14.0.0.bias', 'attn_layers.layers.14.1.to_q.weight', 'attn_layers.layers.14.1.to_k.weight', 'attn_layers.layers.14.1.to_v.weight', 'attn_layers.layers.14.1.to_out.weight', 'attn_layers.layers.15.0.0.weight', 'attn_layers.layers.15.0.0.bias', 'attn_layers.layers.15.1.net.0.0.weight', 'attn_layers.layers.15.1.net.0.0.bias', 'attn_layers.layers.15.1.net.3.weight', 'attn_layers.layers.15.1.net.3.bias', 'attn_layers.layers.16.0.0.weight', 'attn_layers.layers.16.0.0.bias', 'attn_layers.layers.16.1.to_q.weight', 'attn_layers.layers.16.1.to_k.weight', 'attn_layers.layers.16.1.to_v.weight', 'attn_layers.layers.16.1.to_out.weight', 'attn_layers.layers.17.0.0.weight', 'attn_layers.layers.17.0.0.bias', 'attn_layers.layers.17.1.net.0.0.weight', 'attn_layers.layers.17.1.net.0.0.bias', 'attn_layers.layers.17.1.net.3.weight', 'attn_layers.layers.17.1.net.3.bias', 'attn_layers.layers.18.0.0.weight', 'attn_layers.layers.18.0.0.bias', 'attn_layers.layers.18.1.to_q.weight', 'attn_layers.layers.18.1.to_k.weight', 'attn_layers.layers.18.1.to_v.weight', 'attn_layers.layers.18.1.to_out.weight', 'attn_layers.layers.19.0.0.weight', 'attn_layers.layers.19.0.0.bias', 'attn_layers.layers.19.1.net.0.0.weight', 'attn_layers.layers.19.1.net.0.0.bias', 'attn_layers.layers.19.1.net.3.weight', 'attn_layers.layers.19.1.net.3.bias', 'attn_layers.layers.20.0.0.weight', 'attn_layers.layers.20.0.0.bias', 'attn_layers.layers.20.1.to_q.weight', 'attn_layers.layers.20.1.to_k.weight', 'attn_layers.layers.20.1.to_v.weight', 'attn_layers.layers.20.1.to_out.weight', 'attn_layers.layers.21.0.0.weight', 'attn_layers.layers.21.0.0.bias', 'attn_layers.layers.21.1.net.0.0.weight', 'attn_layers.layers.21.1.net.0.0.bias', 'attn_layers.layers.21.1.net.3.weight', 'attn_layers.layers.21.1.net.3.bias', 'attn_layers.layers.22.0.0.weight', 'attn_layers.layers.22.0.0.bias', 'attn_layers.layers.22.1.to_q.weight', 'attn_layers.layers.22.1.to_k.weight', 'attn_layers.layers.22.1.to_v.weight', 'attn_layers.layers.22.1.to_out.weight', 'attn_layers.layers.23.0.0.weight', 'attn_layers.layers.23.0.0.bias', 'attn_layers.layers.23.1.net.0.0.weight', 'attn_layers.layers.23.1.net.0.0.bias', 'attn_layers.layers.23.1.net.3.weight', 'attn_layers.layers.23.1.net.3.bias', 'attn_layers.layers.24.0.0.weight', 'attn_layers.layers.24.0.0.bias', 'attn_layers.layers.24.1.to_q.weight', 'attn_layers.layers.24.1.to_k.weight', 'attn_layers.layers.24.1.to_v.weight', 'attn_layers.layers.24.1.to_out.weight', 'attn_layers.layers.25.0.0.weight', 'attn_layers.layers.25.0.0.bias', 'attn_layers.layers.25.1.net.0.0.weight', 'attn_layers.layers.25.1.net.0.0.bias', 'attn_layers.layers.25.1.net.3.weight', 'attn_layers.layers.25.1.net.3.bias', 'attn_layers.layers.26.0.0.weight', 'attn_layers.layers.26.0.0.bias', 'attn_layers.layers.26.1.to_q.weight', 'attn_layers.layers.26.1.to_k.weight', 'attn_layers.layers.26.1.to_v.weight', 'attn_layers.layers.26.1.to_out.weight', 'attn_layers.layers.27.0.0.weight', 'attn_layers.layers.27.0.0.bias', 'attn_layers.layers.27.1.net.0.0.weight', 'attn_layers.layers.27.1.net.0.0.bias', 'attn_layers.layers.27.1.net.3.weight', 'attn_layers.layers.27.1.net.3.bias', 'attn_layers.layers.28.0.0.weight', 'attn_layers.layers.28.0.0.bias', 'attn_layers.layers.28.1.to_q.weight', 'attn_layers.layers.28.1.to_k.weight', 'attn_layers.layers.28.1.to_v.weight', 'attn_layers.layers.28.1.to_out.weight', 'attn_layers.layers.29.0.0.weight', 'attn_layers.layers.29.0.0.bias', 'attn_layers.layers.29.1.net.0.0.weight', 'attn_layers.layers.29.1.net.0.0.bias', 'attn_layers.layers.29.1.net.3.weight', 'attn_layers.layers.29.1.net.3.bias', 'attn_layers.layers.30.0.0.weight', 'attn_layers.layers.30.0.0.bias', 'attn_layers.layers.30.1.to_q.weight', 'attn_layers.layers.30.1.to_k.weight', 'attn_layers.layers.30.1.to_v.weight', 'attn_layers.layers.30.1.to_out.weight', 'attn_layers.layers.31.0.0.weight', 'attn_layers.layers.31.0.0.bias', 'attn_layers.layers.31.1.net.0.0.weight', 'attn_layers.layers.31.1.net.0.0.bias', 'attn_layers.layers.31.1.net.3.weight', 'attn_layers.layers.31.1.net.3.bias', 'attn_layers.rotary_pos_emb.inv_freq', 'norm.weight', 'norm.bias', 'to_logits.weight', 'to_logits.bias'])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for TransformerWrapper:\n\tsize mismatch for token_emb.emb.weight: copying a param with shape torch.Size([3000, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for to_logits.weight: copying a param with shape torch.Size([3000, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for to_logits.bias: copying a param with shape torch.Size([3000]) from checkpoint, the shape in current model is torch.Size([128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/tedlium/model_utils.py:20\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(args, model, optim, force_cpu)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 20\u001b[0m     model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mmodel_state_dict\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     21\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransformerWrapper:\n\tsize mismatch for token_emb.emb.weight: copying a param with shape torch.Size([3000, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for to_logits.weight: copying a param with shape torch.Size([3000, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for to_logits.bias: copying a param with shape torch.Size([3000]) from checkpoint, the shape in current model is torch.Size([128]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m epoch, val_loss  \u001b[38;5;241m=\u001b[39m model_utils\u001b[38;5;241m.\u001b[39mload_checkpoint(args\u001b[38;5;241m=\u001b[39margsclass(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints_15_mediumplus_tlm3000vocab/checkpoint_7_id_13.pt\u001b[39m\u001b[38;5;124m'\u001b[39m}), model\u001b[38;5;241m=\u001b[39mmodel, force_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/exp/exp1/acp21rjf/deliberation/speachy/tedlium/model_utils.py:22\u001b[0m, in \u001b[0;36mload_checkpoint\u001b[0;34m(args, model, optim, force_cpu)\u001b[0m\n\u001b[1;32m     20\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(checkpoint[\u001b[39m'\u001b[39m\u001b[39mmodel_state_dict\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m---> 22\u001b[0m     model\u001b[39m.\u001b[39;49mload_state_dict(checkpoint[\u001b[39m'\u001b[39;49m\u001b[39mmodel_state_dict\u001b[39;49m\u001b[39m'\u001b[39;49m], strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mError loading model state_dict: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m, loading attempted with strict=False\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mno_load_optim\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m args\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mno_load_optim \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/store/store1/software/bin/anaconda3/envs/k2_custom-nemo/lib/python3.8/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for TransformerWrapper:\n\tsize mismatch for token_emb.emb.weight: copying a param with shape torch.Size([3000, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for to_logits.weight: copying a param with shape torch.Size([3000, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for to_logits.bias: copying a param with shape torch.Size([3000]) from checkpoint, the shape in current model is torch.Size([128])."
     ]
    }
   ],
   "source": [
    "epoch, val_loss  = model_utils.load_checkpoint(args=argsclass(**{'checkpoint': './checkpoints_15_mediumplus_tlm3000vocab/checkpoint_7_id_13.pt'}), model=model, force_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a prominent limitation of the largest clons o'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.greedy_generate('A prominent limitation of ', tokenizer=tokenizer, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lm_utils' from '/exp/exp1/acp21rjf/deliberation/speachy/tedlium/lm_utils.py'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import non_iid_dataloader as niiddl\n",
    "import lm_utils\n",
    "rl(niiddl)\n",
    "rl(lm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = [\n",
    "    0.0,\n",
    "    15.0,\n",
    "    30.0,\n",
    "    45.0,\n",
    "    60.0,\n",
    "    75.0,\n",
    "    90.0,\n",
    "    100.0,\n",
    "    120.0,\n",
    "    140.0,\n",
    "    180.0,\n",
    "    200.0,\n",
    "    250.0,\n",
    "    300.0,\n",
    "    400.0,\n",
    "    500.0,\n",
    "]\n",
    "for duration in durations:\n",
    "    dl = niiddl.get_data_loader(\n",
    "        corpus['test'], \n",
    "        tokenizer=tokenizer, \n",
    "        batch_size=100, \n",
    "        shuffle=False,\n",
    "        max_duration=duration,\n",
    "        text_only=True,\n",
    "    )\n",
    "    ppl, avg_len = lm_utils.eval_corpus_perplexity(model, dl, device='cpu')\n",
    "    print(f\"Duration: {duration}, PPL: {ppl}, Avg Len: {avg_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = niiddl.get_data_loader(\n",
    "    corpus['train'], \n",
    "    tokenizer=tokenizer, \n",
    "    batch_size=20, \n",
    "    shuffle=True,\n",
    "    max_duration=100,\n",
    "    text_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dl:\n",
    "    z = batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = lm_utils.eval_corpus_perplexity(model, dl, device='cpu')\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "perplexity: 417.11: 100%|██████████| 26/26 [01:22<00:00,  3.17s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479.1307067871094 1441.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ppl, avg_len = lm_utils.eval_corpus_perplexity(model, dl, device='cpu')\n",
    "#print(ppl, avg_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "perplexity: 459.58: 100%|██████████| 124/124 [00:18<00:00,  6.71it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "167.10165405273438"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_utils.eval_corpus_perplexity(model, dl, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 17, 54, 97, 3, 107, 3, 107, 3, 107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'⁇  how are as as as'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_utils.greedy_generate(model, tokenizer, 'how are', 10, force_cpu=True, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'lm_utils' from '/exp/exp1/acp21rjf/deliberation/speachy/tedlium/lm_utils.py'>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl(lm_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78, 30, 104, 6, 8]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.text_to_ids('hello there')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens(['<sos>', '<eos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.additional_special_tokens_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_text(out[0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      2\u001b[0m token_lens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = torch.randn(2,3)\n",
    "token_lens = torch.tensor([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_eos(tkn, tkn_len, eos_id = 30):\n",
    "    tkn = tkn.clone() \n",
    "    tkn[torch.arange(tkn.shape[0], device=tkn_len.device, dtype=torch.int32), (tkn_len-1.0).to(torch.int32)] = eos_id\n",
    "    return tkn\n",
    "\n",
    "token_lens.to(torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 78, 30, 104]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0] + tokenizer.text_to_ids('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tensors used as indices must be long, byte or bool tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m add_eos(tokens, token_lens)\n",
      "Cell \u001b[0;32mIn [177], line 3\u001b[0m, in \u001b[0;36madd_eos\u001b[0;34m(tkn, tkn_len, eos_id)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_eos\u001b[39m(tkn, tkn_len, eos_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m      2\u001b[0m     tkn \u001b[38;5;241m=\u001b[39m tkn\u001b[38;5;241m.\u001b[39mclone() \n\u001b[0;32m----> 3\u001b[0m     \u001b[43mtkn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtkn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtkn_len\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtkn_len\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m eos_id\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tkn\n",
      "\u001b[0;31mIndexError\u001b[0m: tensors used as indices must be long, byte or bool tensors"
     ]
    }
   ],
   "source": [
    "add_eos(tokens, token_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt = torch.tensor(\n",
    "    [[0,0,0],\n",
    "    [0,0,0]]\n",
    ", dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3837,  0.5211, -1.2230,  0.5048, -0.1723, -2.0018])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[~bt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lm.s4 import S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  =S4(d_model=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = torch.nn.Embedding(100, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb(torch.randint(0, 100, (2,3))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl(lm_utils)\n",
    "from lm_utils import S4adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.381408"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_params(\n",
    "    S4adapter(S4(d_model=712, measure='legs', mode='nplr', transposed=False, d_state=64), vocab_size=128)\n",
    ") / 1e6 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = S4adapter(S4(d_model=2048, measure='legs', mode='nplr', transposed=False, d_state=512), vocab_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S4adapter(\n",
       "  (model): S4(\n",
       "    (kernel): SSKernel(\n",
       "      (kernel): SSKernelNPLR()\n",
       "    )\n",
       "    (activation): GELU()\n",
       "    (dropout): Identity()\n",
       "    (output_linear): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "      (1): GLU(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (embedding): Embedding(128, 2048)\n",
       "  (predict): Linear(in_features=2048, out_features=128, bias=True)\n",
       "  (softmax): Softmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2048]) emb\n",
      "torch.Size([2, 3, 2048]) s4\n",
      "torch.Size([2, 3, 128]) logits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 128])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model(torch.randint(0, 100, (2,3))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, 100, (2,3)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
