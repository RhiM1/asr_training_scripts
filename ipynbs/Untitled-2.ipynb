{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "import numpy as np\n",
    "from einops import rearrange, repeat\n",
    "from torch import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.Size([5, 9, 94, 32, 64]) dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dots = torch.randn(5, 9, 94, 32, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 3008, 3008])\n",
      "torch.Size([5, 9, 94, 32, 3008])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'km' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m pos \u001b[38;5;241m=\u001b[39m rearrange(pos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h (n i) j -> b h n i j\u001b[39m\u001b[38;5;124m'\u001b[39m, i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(pos\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m pos \u001b[38;5;241m=\u001b[39m pos[repeat(km[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h nw n -> b h nw i n\u001b[39m\u001b[38;5;124m'\u001b[39m, i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# rshape to dots shape\u001b[39;00m\n\u001b[1;32m     10\u001b[0m pos \u001b[38;5;241m=\u001b[39m pos\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39mdots\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'km' is not defined"
     ]
    }
   ],
   "source": [
    "Dpos = DynamicPositionBias(219, heads = 9, depth = 2)\n",
    "pos = Dpos(3008, device = 'cpu', dtype = torch.float32)\n",
    "print(pos.shape)\n",
    "# duplicate for batch\n",
    "pos = repeat(pos, 'h i j -> b h i j', b = 5)\n",
    "pos = rearrange(pos, 'b h (n i) j -> b h n i j', i = 32)\n",
    "print(pos.shape)\n",
    "pos = pos[repeat(km[0], 'b h nw n -> b h nw i n', i = 32)]\n",
    "# rshape to dots shape\n",
    "pos = pos.reshape(*dots.shape)\n",
    "print(pos.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 9, 94, 32, 3008])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.reshape(5, 9, 94, 32, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'km' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pos[repeat(km[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb h nw n -> b h nw i n\u001b[39m\u001b[38;5;124m'\u001b[39m, i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'km' is not defined"
     ]
    }
   ],
   "source": [
    "pos[repeat(km[0], 'b h nw n -> b h nw i n', i = 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'km' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m km\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'km' is not defined"
     ]
    }
   ],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 93, 93, 93],\n",
       "        [ 1,  1,  1,  ..., 92, 92, 92],\n",
       "        [ 2,  2,  2,  ..., 91, 91, 91],\n",
       "        ...,\n",
       "        [91, 91, 91,  ...,  2,  2,  2],\n",
       "        [92, 92, 92,  ...,  1,  1,  1],\n",
       "        [93, 93, 93,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChunkGrid(3008, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(Total_Size, Block_Size):\n",
    "    Psize = Total_Size // Block_Size\n",
    "    chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "    #chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "    \n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4629)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,   10,   11,\n",
       "          12,   13,   14,   15,   16,   17,   18,   19,   20,   21,   22,   23,\n",
       "          24,   25,   26,   27,   28,   29,   30,   31,   32,   33,   34,   35,\n",
       "          36,   37,   38,   39,   40,   41,   42,   43,   44,   45,   46,   47,\n",
       "          48,   49,   50,   51,   52,   53,   54,   55,   56,   57,   58,   59,\n",
       "          60,   61,   62,   63,   64,   65,   66,   67,   68,   69,   70,   71,\n",
       "          72,   73,   74,   75,   76,   77,   78,   79,   80,   81,   82,   83,\n",
       "          84,   85,   86,   87,   88,   89,   90,   91,   92,   93,   94,   95,\n",
       "          96,   97,   98,   99,  100,  101,  102,  103,  104,  105,  106,  107,\n",
       "         108,  109,  110,  111,  112,  113,  114,  115,  116,  117,  118,  119,\n",
       "         120,  121,  122,  123,  124,  125,  126,  127,  128,  129,  130,  131,\n",
       "         132,  133,  134,  135,  136,  137,  138,  139,  140,  141,  142,  143,\n",
       "         144,  145,  146,  147,  148,  149,  150,  151,  152,  153,  154,  155,\n",
       "         156,  157,  158,  159,  160,  161,  162,  163,  164,  165,  166,  167,\n",
       "         168,  169,  170,  171,  172,  173,  174,  175,  176,  177,  178,  179,\n",
       "         180,  181,  182,  183,  184,  185,  186,  187,  188,  189,  190,  191,\n",
       "         192,  193,  194,  195,  196,  197,  198,  199,  200,  201,  202,  203,\n",
       "         204,  205,  206,  207,  208,  209,  210,  211,  212,  213,  214,  215,\n",
       "         216,  217,  218,  219,  220,  221,  222,  223,  224,  225,  226,  227,\n",
       "         228,  229,  230,  231,  232,  233,  234,  235,  236,  237,  238,  239,\n",
       "         240,  244,  246,  247,  248,  250,  253,  254,  255,  256,  257,  258,\n",
       "         259,  262,  263,  264,  265,  266,  267,  268,  269,  270,  271,  273,\n",
       "         275,  276,  277,  278,  280,  281,  283,  284,  285,  287,  288,  290,\n",
       "         291,  293,  295,  296,  299,  302,  303,  304,  308,  311,  312,  320,\n",
       "         321,  324,  325,  327,  328,  330,  337,  342,  343,  345,  354,  357,\n",
       "         359,  367,  368,  369,  370,  372,  380,  386,  393,  398,  399,  405,\n",
       "         406,  411,  412,  419,  425,  427,  439,  444,  451,  457,  458,  464,\n",
       "         470,  472,  473,  500,  509,  512,  514,  517,  519,  527,  537,  538,\n",
       "         551,  556,  558,  565,  568,  620,  637,  640,  667,  669,  675,  682,\n",
       "         697,  736,  740,  775,  794,  795,  820,  831,  882,  896,  897,  918,\n",
       "         924, 1148, 1167, 1177, 1190, 1193, 1248, 1329, 1453, 1487, 1581, 1598,\n",
       "        1611, 1703, 1832, 1851, 2174, 2406, 2637, 2687, 2862, 3444, 3703, 4629])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkgrid = ChunkGrid(4800, 48)\n",
    "pareto = torch.distributions.pareto.Pareto(torch.tensor(3.0), torch.tensor(2.0)).sample(chunkgrid.shape)\n",
    "chunkgrid = chunkgrid - pareto\n",
    "\n",
    "column = 0\n",
    "print(chunkgrid[column].topk(384, largest=False).indices.max())\n",
    "chunkgrid[column].topk(384, largest=False).indices.sort(-1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 4800])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunkgrid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAc00lEQVR4nO3dfXBV9Z348U8gJEQhQXFJSE0gtbb4XAXFiPswNbOMy7RamVZn6A5qq32ILUinCm3BdVsM2l1LdS2s1qKdaql2CvVhq+tGS8ctoqJY2bZAVypMNaGdlgSxBCf5/v7w551GWGsw+YZcXq+ZOyPnnJx8vhzm5u19yC1JKaUAAMhk2GAPAAAcWsQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkVTrYA7xZT09PvPTSSzF69OgoKSkZ7HEAgLchpRS7du2K2traGDbsrR/bOOji46WXXoq6urrBHgMAOADbt2+Po48++i2POejiY/To0RHx+vCVlZWDPA0A8HZ0dnZGXV1d4ef4Wzno4uONp1oqKyvFBwAMMW/nJRNecAoAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyKp0sAfIbeL8Bwd7hD77zZIZgz0CAPQbj3wAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWfUpPrq7u2PhwoXR0NAQFRUVccwxx8RXvvKVSCkVjkkpxaJFi2L8+PFRUVERTU1NsWXLln4fHAAYmvoUH9dff30sW7Ys/u3f/i1++ctfxvXXXx833HBD3HzzzYVjbrjhhrjpppti+fLlsW7dujj88MNj+vTpsWfPnn4fHgAYekr7cvDPfvazOO+882LGjBkRETFx4sT43ve+F08++WREvP6ox9KlS+PLX/5ynHfeeRER8Z3vfCeqq6tj9erVcdFFF/Xz+ADAUNOnRz7OOuusaG1tjc2bN0dExHPPPRePP/54nHvuuRERsXXr1mhra4umpqbC11RVVcXUqVNj7dq1+z1nV1dXdHZ29roBAMWrT498zJ8/Pzo7O2PSpEkxfPjw6O7ujsWLF8esWbMiIqKtrS0iIqqrq3t9XXV1dWHfm7W0tMS11157ILMDAENQnx75uOeee+Kuu+6Ku+++O5555pm4884741/+5V/izjvvPOABFixYEB0dHYXb9u3bD/hcAMDBr0+PfHzhC1+I+fPnF167cdJJJ8WLL74YLS0tMXv27KipqYmIiPb29hg/fnzh69rb2+P973//fs9ZXl4e5eXlBzg+ADDU9OmRj1dffTWGDev9JcOHD4+enp6IiGhoaIiamppobW0t7O/s7Ix169ZFY2NjP4wLAAx1fXrk44Mf/GAsXrw46uvr44QTTohnn302brzxxrj00ksjIqKkpCTmzp0bX/3qV+PYY4+NhoaGWLhwYdTW1sb5558/EPMDAENMn+Lj5ptvjoULF8ZnPvOZ2LFjR9TW1sYnP/nJWLRoUeGYq666Knbv3h2XX3557Ny5M84+++x46KGHYuTIkf0+PAAw9JSkP//1pAeBzs7OqKqqio6OjqisrOz380+c/2C/n3Og/WbJjMEeAQDeUl9+fvtsFwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZNXn+Pjtb38bH/vYx2Ls2LFRUVERJ510Ujz99NOF/SmlWLRoUYwfPz4qKiqiqakptmzZ0q9DAwBDV5/i449//GNMmzYtRowYET/+8Y/jF7/4Rfzrv/5rHHHEEYVjbrjhhrjpppti+fLlsW7dujj88MNj+vTpsWfPnn4fHgAYekr7cvD1118fdXV1sWLFisK2hoaGwn+nlGLp0qXx5S9/Oc4777yIiPjOd74T1dXVsXr16rjooov6aWwAYKjq0yMf9913X0yZMiU+8pGPxLhx4+LUU0+N2267rbB/69at0dbWFk1NTYVtVVVVMXXq1Fi7du1+z9nV1RWdnZ29bgBA8epTfLzwwguxbNmyOPbYY+Phhx+OT3/60/G5z30u7rzzzoiIaGtri4iI6urqXl9XXV1d2PdmLS0tUVVVVbjV1dUdyDoAgCGiT/HR09MTp512Wlx33XVx6qmnxuWXXx6XXXZZLF++/IAHWLBgQXR0dBRu27dvP+BzAQAHvz7Fx/jx4+P444/vte24446Lbdu2RURETU1NRES0t7f3Oqa9vb2w783Ky8ujsrKy1w0AKF59io9p06bFpk2bem3bvHlzTJgwISJef/FpTU1NtLa2FvZ3dnbGunXrorGxsR/GBQCGuj692+XKK6+Ms846K6677rr46Ec/Gk8++WTceuutceutt0ZERElJScydOze++tWvxrHHHhsNDQ2xcOHCqK2tjfPPP38g5gcAhpg+xcfpp58eq1atigULFsQ///M/R0NDQyxdujRmzZpVOOaqq66K3bt3x+WXXx47d+6Ms88+Ox566KEYOXJkvw8PAAw9JSmlNNhD/LnOzs6oqqqKjo6OAXn9x8T5D/b7OQfab5bMGOwRAOAt9eXnt892AQCyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALJ6R/GxZMmSKCkpiblz5xa27dmzJ5qbm2Ps2LExatSomDlzZrS3t7/TOQGAInHA8fHUU0/Fv//7v8fJJ5/ca/uVV14Z999/f9x7772xZs2aeOmll+KCCy54x4MCAMXhgOLjlVdeiVmzZsVtt90WRxxxRGF7R0dH3H777XHjjTfGBz7wgZg8eXKsWLEifvazn8UTTzzRb0MDAEPXAcVHc3NzzJgxI5qamnptX79+fbz22mu9tk+aNCnq6+tj7dq1+z1XV1dXdHZ29roBAMWrtK9fsHLlynjmmWfiqaee2mdfW1tblJWVxZgxY3ptr66ujra2tv2er6WlJa699tq+jgEADFF9euRj+/btMWfOnLjrrrti5MiR/TLAggULoqOjo3Dbvn17v5wXADg49Sk+1q9fHzt27IjTTjstSktLo7S0NNasWRM33XRTlJaWRnV1dezduzd27tzZ6+va29ujpqZmv+csLy+PysrKXjcAoHj16WmXc845J55//vle2y655JKYNGlSXH311VFXVxcjRoyI1tbWmDlzZkREbNq0KbZt2xaNjY39NzUAMGT1KT5Gjx4dJ554Yq9thx9+eIwdO7aw/eMf/3jMmzcvjjzyyKisrIzPfvaz0djYGGeeeWb/TQ0ADFl9fsHpX/L1r389hg0bFjNnzoyurq6YPn16fPOb3+zvbwMADFElKaU02EP8uc7OzqiqqoqOjo4Bef3HxPkP9vs5B9pvlswY7BEA4C315ee3z3YBALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBWfYqPlpaWOP3002P06NExbty4OP/882PTpk29jtmzZ080NzfH2LFjY9SoUTFz5sxob2/v16EBgKGrT/GxZs2aaG5ujieeeCIeeeSReO211+Lv//7vY/fu3YVjrrzyyrj//vvj3nvvjTVr1sRLL70UF1xwQb8PDgAMTaV9Ofihhx7q9ec77rgjxo0bF+vXr4+/+Zu/iY6Ojrj99tvj7rvvjg984AMREbFixYo47rjj4oknnogzzzyz/yYHAIakd/Saj46OjoiIOPLIIyMiYv369fHaa69FU1NT4ZhJkyZFfX19rF27dr/n6Orqis7Ozl43AKB4HXB89PT0xNy5c2PatGlx4oknRkREW1tblJWVxZgxY3odW11dHW1tbfs9T0tLS1RVVRVudXV1BzoSADAEHHB8NDc3x8aNG2PlypXvaIAFCxZER0dH4bZ9+/Z3dD4A4ODWp9d8vOGKK66IBx54IH7605/G0UcfXdheU1MTe/fujZ07d/Z69KO9vT1qamr2e67y8vIoLy8/kDEAgCGoT498pJTiiiuuiFWrVsWjjz4aDQ0NvfZPnjw5RowYEa2trYVtmzZtim3btkVjY2P/TAwADGl9euSjubk57r777vjRj34Uo0ePLryOo6qqKioqKqKqqio+/vGPx7x58+LII4+MysrK+OxnPxuNjY3e6QIAREQf42PZsmUREfF3f/d3vbavWLEiLr744oiI+PrXvx7Dhg2LmTNnRldXV0yfPj2++c1v9suwAMDQ16f4SCn9xWNGjhwZt9xyS9xyyy0HPBQAULx8tgsAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBkJT4AgKzEBwCQlfgAALISHwBAVuIDAMhKfAAAWYkPACAr8QEAZCU+AICsxAcAkJX4AACyKh3sAfjLJs5/cLBH6LPfLJkx2CMAcJDyyAcAkJX4AACyEh8AQFbiAwDISnwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBWPtsF/j+foQOQh0c+AICsxAcAkNWAPe1yyy23xNe+9rVoa2uLU045JW6++eY444wzBurbAcCg8JRt3w3IIx/f//73Y968eXHNNdfEM888E6ecckpMnz49duzYMRDfDgAYQgYkPm688ca47LLL4pJLLonjjz8+li9fHocddlh8+9vfHohvBwAMIf3+tMvevXtj/fr1sWDBgsK2YcOGRVNTU6xdu3af47u6uqKrq6vw546OjoiI6Ozs7O/RIiKip+vVATkvvQ3U9RtIQ/HfxlD8e4Zi476j9zlTSn/x2H6Pj9///vfR3d0d1dXVvbZXV1fHr371q32Ob2lpiWuvvXaf7XV1df09GhlVLR3sCQ4N/p6BAzGQ9x27du2Kqqqqtzxm0H/Px4IFC2LevHmFP/f09MQf/vCHGDt2bJSUlETE6zVVV1cX27dvj8rKysEaNZtDab2H0lojDq31HkprjTi01nsorTXCet+ulFLs2rUramtr/+Kx/R4fRx11VAwfPjza29t7bW9vb4+ampp9ji8vL4/y8vJe28aMGbPfc1dWVh4SF/4Nh9J6D6W1Rhxa6z2U1hpxaK33UFprhPW+HX/pEY839PsLTsvKymLy5MnR2tpa2NbT0xOtra3R2NjY398OABhiBuRpl3nz5sXs2bNjypQpccYZZ8TSpUtj9+7dcckllwzEtwMAhpABiY8LL7wwfve738WiRYuira0t3v/+98dDDz20z4tQ367y8vK45ppr9nl6plgdSus9lNYacWit91Baa8Shtd5Daa0R1jsQStLbeU8MAEA/8dkuAEBW4gMAyEp8AABZiQ8AIKuDPj5uueWWmDhxYowcOTKmTp0aTz755GCP1C9++tOfxgc/+MGora2NkpKSWL16da/9KaVYtGhRjB8/PioqKqKpqSm2bNkyOMO+Qy0tLXH66afH6NGjY9y4cXH++efHpk2beh2zZ8+eaG5ujrFjx8aoUaNi5syZ+/yiuqFi2bJlcfLJJxd+QU9jY2P8+Mc/LuwvprW+2ZIlS6KkpCTmzp1b2FZM6/2nf/qnKCkp6XWbNGlSYX8xrfUNv/3tb+NjH/tYjB07NioqKuKkk06Kp59+urC/WO6rJk6cuM+1LSkpiebm5ogovmvb3d0dCxcujIaGhqioqIhjjjkmvvKVr/T6XJYBvbbpILZy5cpUVlaWvv3tb6f/+Z//SZdddlkaM2ZMam9vH+zR3rH/+I//SF/60pfSD3/4wxQRadWqVb32L1myJFVVVaXVq1en5557Ln3oQx9KDQ0N6U9/+tPgDPwOTJ8+Pa1YsSJt3LgxbdiwIf3DP/xDqq+vT6+88krhmE996lOprq4utba2pqeffjqdeeaZ6ayzzhrEqQ/cfffdlx588MG0efPmtGnTpvTFL34xjRgxIm3cuDGlVFxr/XNPPvlkmjhxYjr55JPTnDlzCtuLab3XXHNNOuGEE9LLL79cuP3ud78r7C+mtaaU0h/+8Ic0YcKEdPHFF6d169alF154IT388MPp17/+deGYYrmv2rFjR6/r+sgjj6SISI899lhKqfiu7eLFi9PYsWPTAw88kLZu3ZruvffeNGrUqPSNb3yjcMxAXtuDOj7OOOOM1NzcXPhzd3d3qq2tTS0tLYM4Vf97c3z09PSkmpqa9LWvfa2wbefOnam8vDx973vfG4QJ+9eOHTtSRKQ1a9aklF5f24gRI9K9995bOOaXv/xlioi0du3awRqzXx1xxBHpW9/6VtGuddeuXenYY49NjzzySPrbv/3bQnwU23qvueaadMopp+x3X7GtNaWUrr766nT22Wf/n/uL+b5qzpw56Zhjjkk9PT1FeW1nzJiRLr300l7bLrjggjRr1qyU0sBf24P2aZe9e/fG+vXro6mpqbBt2LBh0dTUFGvXrh3EyQbe1q1bo62trdfaq6qqYurUqUWx9o6OjoiIOPLIIyMiYv369fHaa6/1Wu+kSZOivr5+yK+3u7s7Vq5cGbt3747GxsaiXWtzc3PMmDGj17oiivPabtmyJWpra+Pd7353zJo1K7Zt2xYRxbnW++67L6ZMmRIf+chHYty4cXHqqafGbbfdVthfrPdVe/fuje9+97tx6aWXRklJSVFe27POOitaW1tj8+bNERHx3HPPxeOPPx7nnntuRAz8tR30T7X9v/z+97+P7u7ufX4ranV1dfzqV78apKnyaGtri4jY79rf2DdU9fT0xNy5c2PatGlx4oknRsTr6y0rK9vnAwWH8nqff/75aGxsjD179sSoUaNi1apVcfzxx8eGDRuKbq0rV66MZ555Jp566ql99hXbtZ06dWrccccd8b73vS9efvnluPbaa+Ov//qvY+PGjUW31oiIF154IZYtWxbz5s2LL37xi/HUU0/F5z73uSgrK4vZs2cX7X3V6tWrY+fOnXHxxRdHRPH9O46ImD9/fnR2dsakSZNi+PDh0d3dHYsXL45Zs2ZFxMD/HDpo44Pi1NzcHBs3bozHH398sEcZUO973/tiw4YN0dHRET/4wQ9i9uzZsWbNmsEeq99t37495syZE4888kiMHDlysMcZcG/8X2FExMknnxxTp06NCRMmxD333BMVFRWDONnA6OnpiSlTpsR1110XERGnnnpqbNy4MZYvXx6zZ88e5OkGzu233x7nnnvu2/po+KHqnnvuibvuuivuvvvuOOGEE2LDhg0xd+7cqK2tzXJtD9qnXY466qgYPnz4Pq8mbm9vj5qamkGaKo831ldsa7/iiivigQceiMceeyyOPvrowvaamprYu3dv7Ny5s9fxQ3m9ZWVl8Z73vCcmT54cLS0tccopp8Q3vvGNolvr+vXrY8eOHXHaaadFaWlplJaWxpo1a+Kmm26K0tLSqK6uLqr1vtmYMWPive99b/z6178uumsbETF+/Pg4/vjje2077rjjCk81FeN91Ysvvhj/9V//FZ/4xCcK24rx2n7hC1+I+fPnx0UXXRQnnXRS/OM//mNceeWV0dLSEhEDf20P2vgoKyuLyZMnR2tra2FbT09PtLa2RmNj4yBONvAaGhqipqam19o7Oztj3bp1Q3LtKaW44oorYtWqVfHoo49GQ0NDr/2TJ0+OESNG9Frvpk2bYtu2bUNyvfvT09MTXV1dRbfWc845J55//vnYsGFD4TZlypSYNWtW4b+Lab1v9sorr8T//u//xvjx44vu2kZETJs2bZ+3xW/evDkmTJgQEcV3XxURsWLFihg3blzMmDGjsK0Yr+2rr74aw4b1ToDhw4dHT09PRGS4tu/4JasDaOXKlam8vDzdcccd6Re/+EW6/PLL05gxY1JbW9tgj/aO7dq1Kz377LPp2WefTRGRbrzxxvTss8+mF198MaX0+lucxowZk370ox+ln//85+m8884bkm9fSymlT3/606mqqir95Cc/6fVWtldffbVwzKc+9alUX1+fHn300fT000+nxsbG1NjYOIhTH7j58+enNWvWpK1bt6af//znaf78+amkpCT953/+Z0qpuNa6P3/+bpeUimu9n//859NPfvKTtHXr1vTf//3fqampKR111FFpx44dKaXiWmtKr799urS0NC1evDht2bIl3XXXXemwww5L3/3udwvHFNN9VXd3d6qvr09XX331PvuK7drOnj07vetd7yq81faHP/xhOuqoo9JVV11VOGYgr+1BHR8ppXTzzTen+vr6VFZWls4444z0xBNPDPZI/eKxxx5LEbHPbfbs2Sml19/mtHDhwlRdXZ3Ky8vTOeeckzZt2jS4Qx+g/a0zItKKFSsKx/zpT39Kn/nMZ9IRRxyRDjvssPThD384vfzyy4M39Dtw6aWXpgkTJqSysrL0V3/1V+mcc84phEdKxbXW/XlzfBTTei+88MI0fvz4VFZWlt71rnelCy+8sNfvvCimtb7h/vvvTyeeeGIqLy9PkyZNSrfeemuv/cV0X/Xwww+niNjv/MV2bTs7O9OcOXNSfX19GjlyZHr3u9+dvvSlL6Wurq7CMQN5bUtS+rNfZwYAMMAO2td8AADFSXwAAFmJDwAgK/EBAGQlPgCArMQHAJCV+AAAshIfAEBW4gMAyEp8AABZiQ8AICvxAQBk9f8AgowrNkWaPmIAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = torch.distributions.pareto.Pareto(3, 2).sample(torch.tensor([100]))\n",
    "plt.hist(dist)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4629)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3703, 1581, 1611, 1851, 1453,  427, 2174,  200,  924,  697, 1148,  412,\n",
       "         538, 4629,  519,  210,  918, 1832,  419, 1167, 1703, 2862,  235, 1193,\n",
       "        2637,  637,   30,  164,  275,  278,  667,   53, 3444,  195,  399,  163,\n",
       "         342,  159, 1598,   27])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(chunkgrid[0].topk(40, largest=False).indices.max())\n",
    "chunkgrid[0].topk(40, largest=False).indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "class DynamicPositionBias(nn.Module):\n",
    "    def __init__(self, dim, *, heads, depth, log_distance = False, norm = False):\n",
    "        super().__init__()\n",
    "        assert depth >= 1, 'depth for dynamic position bias MLP must be greater or equal to 1'\n",
    "        self.log_distance = log_distance\n",
    "\n",
    "        self.mlp = nn.ModuleList([])\n",
    "\n",
    "        self.mlp.append(nn.Sequential(\n",
    "            nn.Linear(1, dim),\n",
    "            nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "            nn.ReLU()\n",
    "        ))\n",
    "\n",
    "        for _ in range(depth - 1):\n",
    "            self.mlp.append(nn.Sequential(\n",
    "                nn.Linear(dim, dim),\n",
    "                nn.LayerNorm(dim) if norm else nn.Identity(),\n",
    "                nn.ReLU()\n",
    "            ))\n",
    "\n",
    "        self.mlp.append(nn.Linear(dim, heads))\n",
    "\n",
    "    def forward(self, n, device, dtype):\n",
    "\n",
    "        # get the (n x n) matrix of distances\n",
    "        seq_arange = torch.arange(n, device = device)\n",
    "        context_arange = torch.arange(n, device = device)\n",
    "        indices = rearrange(seq_arange, 'i -> i 1') - rearrange(context_arange, 'j -> 1 j')\n",
    "        indices += (n - 1)\n",
    "        \n",
    "        # input to continuous positions MLP\n",
    "        pos = torch.arange(-n + 1, n, device = device, dtype = dtype)\n",
    "        pos = rearrange(pos, '... -> ... 1')\n",
    "\n",
    "        if self.log_distance:\n",
    "            pos = torch.sign(pos) * torch.log(pos.abs() + 1)  # log of distance is sign(rel_pos) * log(abs(rel_pos) + 1)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            pos = layer(pos)\n",
    "\n",
    "        # get position biases        \n",
    "        bias = pos[indices]\n",
    "        bias = rearrange(bias, 'i j h -> h i j')\n",
    "        return bias\n",
    "\n",
    "class MyopicAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_feats,\n",
    "        head_dim,\n",
    "        n_heads,\n",
    "        dropout=0.0,\n",
    "        max_keep_keys=50,\n",
    "        chunk_window=3,\n",
    "        bias=True,\n",
    "        return_attention=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_feats = n_feats\n",
    "        self.head_dim = head_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.return_attention = return_attention\n",
    "\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.max_keep_keys = max_keep_keys\n",
    "        self.W = chunk_window\n",
    "\n",
    "        self.positional_bias = DynamicPositionBias(\n",
    "            dim = n_feats,\n",
    "            heads = n_heads,\n",
    "            depth = 2,\n",
    "            log_distance = False,\n",
    "            norm = False\n",
    "        )\n",
    "\n",
    "        self.qkv_proj = nn.Linear(n_feats, 3 * n_heads * head_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(n_heads * head_dim, n_feats, bias=bias)\n",
    "\n",
    "    def pad_to_window_size(self, x, window_size, axis=3, mask=None):\n",
    "        \"\"\"\n",
    "        Pad the input on two sides to be divisible by `window_size`\n",
    "        \"\"\"\n",
    "        QKV, batch_size, heads, sequence_length, hidden_size = x.shape\n",
    "        padding_length = (window_size - sequence_length % window_size) % window_size\n",
    "        padding = torch.zeros(QKV, batch_size, heads, padding_length, hidden_size,\n",
    "            device=x.device,\n",
    "            dtype=x.dtype,\n",
    "        )\n",
    "        mask = F.pad(mask, (0, padding_length), value=True) if exists(mask) else None\n",
    "        return torch.cat([x, padding], axis=axis), padding_length, mask\n",
    "\n",
    "    def unpad(self, x, padding_length):\n",
    "        \"\"\"\n",
    "        Undo padding.\n",
    "        \"\"\"\n",
    "        if padding_length > 0:\n",
    "            return x[:, :-padding_length]\n",
    "        return x\n",
    "\n",
    "    def ChunkGrid(self, Total_Size, Block_Size):\n",
    "        Psize = Total_Size // Block_Size\n",
    "        chunk_grid = (torch.arange(0, Psize).repeat(Psize,1) - torch.arange(0, Psize).repeat(Psize,1).T ).repeat_interleave(Block_Size, dim=1).abs()\n",
    "        chunk_grid = 1 - (chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1))\n",
    "        return chunk_grid    \n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        B, N, C, H, D = *x.shape, self.n_heads, self.head_dim\n",
    "        tokeep = min(self.max_keep_keys, N) if self.max_keep_keys != -1 else N\n",
    "        W = min(self.W, N) if self.W != -1 else N\n",
    "        qkv = rearrange(self.qkv_proj(x), \"B N (H D QKV) -> QKV B H N D\", QKV=3, H=H, D=D)\n",
    "\n",
    "        qkv, pad_n, mask = self.pad_to_window_size(qkv, W, axis=3, mask=mask)\n",
    "        q, kv = qkv[0], qkv[1:]\n",
    "        # split q into windows/chunks of size W\n",
    "        q = rearrange(q, \"B H (N W) D -> B H N W D\", W=W)\n",
    "        if exists(mask):\n",
    "            q_mask = rearrange(mask, \"B (N W) -> B N W\", W=W)\n",
    "            q_mask = repeat(q_mask, \"B N W -> B H N W\", H=H)\n",
    "            \n",
    "        # duplicate k and v for total number of windows\n",
    "        kv = repeat(kv, \"KV B H N D -> KV B H NW N D\", NW=q.shape[2])\n",
    "        #print(q.shape, kv.shape)\n",
    "        KV, B, H, NW, N, D = kv.shape\n",
    "\n",
    "        chunkgrid = self.ChunkGrid(Total_Size=N, Block_Size=W).to(q.device)\n",
    "        \n",
    "        chunkgrid = repeat(chunkgrid, \"W N -> B H W N\", B=B, H=H).contiguous()\n",
    "        MEAN = torch.tensor(0, device=q.device, dtype=q.dtype)\n",
    "        STD = torch.tensor(0.125, device=q.device, dtype=q.dtype)\n",
    "        uniform_dist = torch.distributions.normal.Normal(MEAN, STD).sample(chunkgrid.shape).to(q.device)\n",
    "        chunkgrid += uniform_dist\n",
    "        chunkgrid = repeat(chunkgrid, \"B H W N -> KV B H W N\", KV=2)\n",
    "\n",
    "        print(chunkgrid.shape, 'cg')\n",
    "        cmask = repeat(mask, 'B N -> KV B H NW N', KV=2, H=H, NW=NW)\n",
    "        \n",
    "        chunkgrid.masked_fill_(mask=cmask, value=-torch.finfo(q.dtype).max) ########\n",
    "       \n",
    "        keep_indices = chunkgrid.topk(k=tokeep, dim=-1, sorted=False).indices.sort(dim=-1).values\n",
    "        KV, B, H, NW, N, D = kv.shape \n",
    "        kv = kv.gather(-2, repeat(keep_indices, \"KV B H W N -> KV B H W N D\", D=D))\n",
    "\n",
    "        if exists(mask):\n",
    "            kv_mask = repeat(mask, \"B N -> B H NW N\", H=H, NW=NW).gather(-1, keep_indices[0])\n",
    "\n",
    "        k, v = kv\n",
    "        # NW (number of windows) = P (in below einsum)\n",
    "        dots = einsum(\"B H N P D, B H N Z D -> B H N P Z \", q, k) * self.scale # Z is number of chunks in Q, N is max sequence length after dropping\n",
    "\n",
    "        ## positional stuff\n",
    "        pos_bias = self.positional_bias(N, device=dots.device, dtype=dots.dtype)\n",
    "        pos_bias = repeat(pos_bias, 'H I J -> B H I J', B = B)\n",
    "        pos_bias = rearrange(pos_bias, 'B H (N W) J -> B H N W J', W = W)\n",
    "\n",
    "        pos_bias = pos_bias.gather(-1, repeat(keep_indices, \"KV B H NW N -> KV B H NW W N\", W=W)[0])\n",
    "        \n",
    "        dots = dots + pos_bias\n",
    "\n",
    "        if exists(mask):\n",
    "            mask_val = -torch.finfo(dots.dtype).max\n",
    "            qk_mask = rearrange(q_mask, \"B H N W -> B H N W ()\") * rearrange(kv_mask, \"B H W N -> B H W () N\")\n",
    "            dots.masked_fill_(qk_mask, mask_val)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "  \n",
    "        out = einsum(\"B H N W Z, B H N Z D -> B H N W D\", attn, v)\n",
    "        out = rearrange(out, \"B H N W D -> B (N W) (H D)\")\n",
    "        out = self.unpad(out, pad_n)\n",
    "        out = self.out_proj(out)\n",
    "        return out if not return_attention else (out, attn)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 12, 1, 1000]) cg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1090470/3731934327.py:144: UserWarning: Use of masked_fill_ on expanded tensors is deprecated. Please clone() the tensor before performing this operation. This also applies to advanced indexing e.g. tensor[mask] = scalar (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755903507/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1398.)\n",
      "  chunkgrid.masked_fill_(mask=cmask, value=-torch.finfo(q.dtype).max)\n"
     ]
    }
   ],
   "source": [
    "attention = MyopicAttention(n_feats=216, head_dim=24, n_heads=12, max_keep_keys=-1, chunk_window=-1)\n",
    "\n",
    "attn = attention(x, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mydots' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(mydots\u001b[38;5;241m.\u001b[39mshape, normaldots\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mydots' is not defined"
     ]
    }
   ],
   "source": [
    "print(mydots.shape, normaldots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydots = mydots.permute(0, 3, 4, 1, 2).reshape(*[10, 1000, 288])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woop\n"
     ]
    }
   ],
   "source": [
    "assert torch.allclose(normaldots, mydots.squeeze()), \"we are not the same\"\n",
    "print('woop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(normaldots.reshape(*[10, 1000, 288]), mydots.reshape(*[10, 1000, 288]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1000, 288]) torch.Size([10, 1000, 288])\n"
     ]
    }
   ],
   "source": [
    "normaldots = rearrange(normaldots, 'b h n d -> b n (h d)')\n",
    "#mydots = rearrange(mydots, 'b h w n d -> b (w n) (h d)')\n",
    "mydots = mydots.squeeze()\n",
    "print(normaldots.shape, mydots.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.2519, grad_fn=<SelectBackward0>)\n",
      "tensor(-0.2519, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "second_column =0\n",
    "last_column = 20\n",
    "\n",
    "print(normaldots.reshape(*[10, 1000, 288])[0][second_column][last_column])\n",
    "print(mydots.reshape(*[10, 1000, 288])[0][second_column][last_column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10, 1000, 216)\n",
    "mask = torch.zeros(10, 1000).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([288])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.reshape(o.shape)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1000, 288])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.6628e-02, -3.9733e-03, -1.6485e-01, -1.3621e-01, -8.7017e-02,\n",
       "         1.3378e-01,  4.9637e-03, -4.1894e-03, -7.3796e-02,  8.6580e-03,\n",
       "        -1.3460e-01, -1.0907e-01,  1.7497e-01, -4.9137e-03,  2.2401e-01,\n",
       "         1.4699e-01,  1.7687e-01,  7.0131e-02, -1.3558e-01, -8.2851e-02,\n",
       "        -8.5467e-02, -6.2461e-02, -1.3721e-03, -7.3953e-02,  7.2629e-02,\n",
       "        -6.8547e-02,  5.5272e-02, -3.2858e-02, -3.7293e-02,  5.0426e-02,\n",
       "         3.0691e-02, -1.0333e-02, -1.6956e-02, -3.3131e-02, -1.0702e-01,\n",
       "        -3.0205e-02,  3.5606e-02,  5.2930e-02, -4.2927e-02, -1.7174e-02,\n",
       "        -3.1038e-02, -4.6316e-02,  4.0095e-02, -5.3015e-02, -2.8979e-02,\n",
       "         4.9179e-02,  3.0483e-03,  6.9603e-02, -3.3213e-01, -1.8481e-01,\n",
       "        -1.2169e-01, -2.3707e-02, -7.8052e-02, -4.6261e-02, -9.8207e-02,\n",
       "         2.3057e-01, -8.5172e-02, -8.0997e-02, -2.8267e-01, -2.6922e-01,\n",
       "         1.1706e-01,  3.6214e-01,  7.5635e-04,  1.8275e-01,  1.9320e-01,\n",
       "         4.1853e-01, -2.0188e-01,  2.9349e-01,  2.7642e-01,  2.1770e-01,\n",
       "        -9.4209e-02, -8.1762e-02,  2.9181e-01, -7.8754e-02, -8.6467e-02,\n",
       "        -4.7990e-02, -8.2947e-03,  6.4890e-02,  1.8293e-01, -1.7251e-01,\n",
       "        -3.0704e-01,  2.3248e-02, -2.3635e-01, -8.6515e-02,  8.0689e-02,\n",
       "         1.0399e-01,  2.0871e-02, -1.4414e-01, -1.0950e-01, -7.9734e-02,\n",
       "         7.3275e-03,  1.6459e-01, -3.7228e-03, -1.5798e-01,  1.4250e-01,\n",
       "         8.3870e-02, -1.6866e-03,  7.2642e-02, -5.5224e-02, -9.2941e-03,\n",
       "         2.9187e-02,  4.9501e-02,  6.3121e-02, -1.7309e-03, -1.1523e-02,\n",
       "         7.9610e-02, -3.7687e-03,  4.8867e-03, -6.6321e-02, -4.1929e-03,\n",
       "        -1.4565e-02,  7.1820e-02,  7.0261e-02, -1.4855e-01, -3.2288e-02,\n",
       "        -1.4136e-02,  2.9159e-02, -4.2111e-02, -9.0917e-02, -6.8765e-02,\n",
       "         1.9681e-01, -3.9359e-02,  1.0252e-01, -1.8473e-02, -4.1910e-02,\n",
       "         6.8064e-02, -2.1021e-01,  1.7464e-01,  7.1043e-02,  8.7326e-02,\n",
       "        -1.2913e-01, -5.3169e-02, -2.2328e-01,  1.0230e-02, -2.0077e-01,\n",
       "        -8.9456e-02,  1.0424e-01,  8.7842e-02, -4.0392e-02, -3.6321e-02,\n",
       "         8.8046e-02, -1.4486e-01, -4.4565e-03, -1.7097e-02,  6.0850e-02,\n",
       "        -4.0433e-02, -1.0599e-02,  2.7889e-02, -3.5955e-02,  2.2242e-02,\n",
       "        -5.4429e-02, -8.2053e-02,  1.0951e-01, -3.2606e-02,  4.8997e-02,\n",
       "         1.7988e-02,  5.1692e-02,  9.5633e-02,  1.5142e-02,  6.2746e-02,\n",
       "        -1.4667e-02,  1.0699e-02,  3.9199e-02, -4.5151e-02, -8.0670e-02,\n",
       "         3.1459e-02, -4.5795e-02, -2.5933e-03, -1.0898e-01,  4.7060e-02,\n",
       "        -5.1705e-02,  5.4972e-02, -1.9742e-01,  1.1824e-01,  1.4162e-01,\n",
       "         9.9854e-02, -4.3208e-02,  7.7150e-02,  3.2694e-02, -1.4249e-01,\n",
       "         1.4631e-01, -1.1335e-01,  4.7952e-02, -8.4531e-03, -1.0660e-01,\n",
       "         5.6729e-02,  9.8369e-02,  9.7884e-02, -7.4219e-02,  8.0157e-02,\n",
       "        -8.7558e-02,  1.0960e-01,  3.0024e-01,  9.5920e-02, -6.4501e-02,\n",
       "         2.0626e-01,  1.2421e-01,  3.2892e-02, -7.5270e-02,  9.1057e-02,\n",
       "         1.2469e-01,  1.1230e-04,  4.3410e-03,  1.1064e-01, -8.8301e-03,\n",
       "        -2.9212e-02,  2.3500e-01,  2.5978e-01,  8.3727e-02,  1.3278e-01,\n",
       "         1.1584e-02,  7.3746e-02, -9.4752e-02,  3.2603e-02,  6.7815e-02,\n",
       "         4.1497e-02, -6.1990e-02, -1.2912e-01, -7.1963e-02,  8.8105e-02,\n",
       "        -4.3177e-02, -1.9991e-02,  5.5213e-02, -7.0496e-02, -4.9457e-02,\n",
       "        -1.8449e-02,  7.6128e-02,  2.7707e-02, -5.7557e-02,  1.5302e-01,\n",
       "         1.2654e-01,  6.2070e-02, -7.5875e-02,  3.1542e-02, -1.5179e-01,\n",
       "        -8.4681e-02,  4.1776e-02,  1.3475e-02,  8.7518e-02, -9.3802e-02,\n",
       "        -2.4868e-02,  2.0937e-01,  6.2006e-02,  2.8213e-02, -1.5310e-01,\n",
       "        -1.7296e-01, -1.7322e-02, -1.2301e-01,  1.0401e-01, -1.1084e-01,\n",
       "         5.1436e-02, -1.6989e-01, -2.7514e-01, -1.7817e-01, -6.6537e-02,\n",
       "        -1.3105e-02, -8.4472e-02, -1.4438e-01,  9.9209e-02, -1.7810e-01,\n",
       "         7.8920e-02, -5.8050e-02, -4.9897e-03, -1.2249e-01,  6.1950e-02,\n",
       "         7.8125e-02, -4.6769e-02,  1.1148e-01,  6.2595e-03,  1.1502e-01,\n",
       "         6.0143e-02, -1.2870e-01, -1.4981e-03, -1.0989e-01,  2.4324e-02,\n",
       "         3.4474e-01, -9.8612e-02, -5.0141e-02,  4.2873e-02,  6.3759e-02,\n",
       "        -3.9629e-02,  7.8128e-02,  4.9499e-02, -8.4682e-02,  7.1232e-02,\n",
       "        -9.8612e-03,  8.8940e-02,  2.0436e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0158,  0.0995, -0.0365, -0.1100,  0.0334,  0.0426, -0.0559, -0.0099,\n",
       "        -0.0433, -0.0294,  0.0380,  0.1200,  0.0432, -0.0370,  0.0225,  0.0208,\n",
       "        -0.0936, -0.0647,  0.0883, -0.0521,  0.0226, -0.0264,  0.0386, -0.0356,\n",
       "        -0.0234,  0.0948, -0.0419, -0.0824,  0.0371, -0.0042, -0.0448, -0.0239,\n",
       "        -0.0418, -0.0161,  0.0216,  0.0818,  0.0284, -0.0340, -0.0004,  0.0185,\n",
       "        -0.0729, -0.0705,  0.0572, -0.0349,  0.0030, -0.0103, -0.0111,  0.0069,\n",
       "         0.0322,  0.1101, -0.0391, -0.0739,  0.0429,  0.0121, -0.0368, -0.0178,\n",
       "        -0.0605, -0.0355,  0.0520,  0.0599,  0.0368, -0.0061,  0.0401,  0.0240,\n",
       "        -0.0522, -0.0404,  0.0357, -0.0948, -0.0124, -0.0013, -0.0035, -0.0068,\n",
       "         0.0026,  0.0847, -0.0310, -0.0910,  0.0488,  0.0197, -0.0570, -0.0149,\n",
       "        -0.0368, -0.0208,  0.0392,  0.0997,  0.0276, -0.0124,  0.0138,  0.0275,\n",
       "        -0.0865, -0.0496,  0.0638, -0.0559, -0.0021, -0.0384,  0.0196, -0.0367,\n",
       "        -0.0062,  0.0923, -0.0444, -0.0841,  0.0251,  0.0370, -0.0549,  0.0081,\n",
       "        -0.0266, -0.0088,  0.0362,  0.1019,  0.0173, -0.0177,  0.0076,  0.0102,\n",
       "        -0.0898, -0.0562,  0.0746, -0.0514,  0.0066, -0.0419, -0.0067, -0.0484,\n",
       "         0.0159,  0.1086, -0.0418, -0.1025,  0.0126,  0.0035, -0.0192,  0.0089,\n",
       "        -0.0364, -0.0016,  0.0383,  0.0776,  0.0391, -0.0203,  0.0395,  0.0380,\n",
       "        -0.0565, -0.0686,  0.0272, -0.0492, -0.0214, -0.0137, -0.0114, -0.0062,\n",
       "         0.0032,  0.1053, -0.0633, -0.0872,  0.0401,  0.0435, -0.0472,  0.0060,\n",
       "        -0.0474,  0.0135,  0.0532,  0.0760,  0.0448, -0.0011,  0.0144,  0.0166,\n",
       "        -0.0433, -0.0745,  0.0625, -0.0418,  0.0051, -0.0194,  0.0091, -0.0201,\n",
       "         0.0188,  0.0864, -0.0593, -0.0991,  0.0298,  0.0523, -0.0637, -0.0115,\n",
       "        -0.0512, -0.0047,  0.0456,  0.0846,  0.0479, -0.0099,  0.0259,  0.0197,\n",
       "        -0.0550, -0.0550,  0.0632, -0.0559, -0.0030, -0.0177,  0.0015, -0.0093,\n",
       "        -0.0115,  0.1340, -0.0319, -0.1043,  0.0459,  0.0163, -0.0372, -0.0147,\n",
       "        -0.0222, -0.0174,  0.0478,  0.0481,  0.0447, -0.0428,  0.0184,  0.0325,\n",
       "        -0.0524, -0.0567,  0.0233, -0.0270,  0.0029, -0.0302, -0.0381, -0.0694,\n",
       "         0.0039,  0.1039, -0.0401, -0.0776,  0.0322,  0.0034, -0.0417,  0.0115,\n",
       "        -0.0614, -0.0086,  0.0418,  0.0667,  0.0295, -0.0024,  0.0244,  0.0248,\n",
       "        -0.0338, -0.0798,  0.0418, -0.0706, -0.0058, -0.0058, -0.0008,  0.0113,\n",
       "         0.0208,  0.1204, -0.0605, -0.1029,  0.0051,  0.0034, -0.0076, -0.0272,\n",
       "        -0.0539, -0.0011,  0.0488,  0.0445,  0.0592, -0.0297,  0.0455, -0.0005,\n",
       "        -0.0453, -0.0724,  0.0461, -0.0715, -0.0212, -0.0091, -0.0216,  0.0219,\n",
       "         0.0287,  0.0660, -0.0122, -0.1117,  0.0012,  0.0126, -0.0286, -0.0276,\n",
       "        -0.0161, -0.0105,  0.0236,  0.0931,  0.0246, -0.0310,  0.0396,  0.0284,\n",
       "        -0.0813, -0.0709,  0.0387, -0.0180, -0.0301, -0.0193, -0.0025, -0.0235],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.reshape(o.shape)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 12, 125, 8, 256])"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.indices.sort().values[0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(2).unsqueeze(-1).repeat(2).expand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km[0, 0, :, 0, :100].sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(torch.arange(0, 3008).repeat(3008,1) - torch.arange(0, 3008).repeat(3008,1).T).reshape(32, -1, 3008).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- duplicated across KV\n",
    "- each batch, head and Window have a different view of the keys\n",
    "- 94 is the number of windows i.e 94*32(win size) = 3008 (sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv[km].reshape(2, 5, 8, 3, -1, 24).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeat(cg, \"W N -> KV B H W N\", B=5, H=8, KV=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kv.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChunkGrid(N_BLOCKS, BLOCK_SIZE):\n",
    "    chunk_grid = (torch.arange(0, N_BLOCKS).repeat(BLOCK_SIZE,1) - torch.arange(0, BLOCK_SIZE).repeat(N_BLOCKS,1).T).repeat_interleave(BLOCK_SIZE, dim=1).abs()\n",
    "    chunk_grid = chunk_grid / chunk_grid.max(dim=-1)[0].unsqueeze(-1)\n",
    "    return chunk_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg = ChunkGrid(41, 3)\n",
    "uniform_dist = torch.distributions.uniform.Uniform(0, 1).sample(cg.shape)\n",
    "cg += uniform_dist\n",
    "keep_indices = cg.topk(9, dim=-1).indices\n",
    "keep_mask = torch.zeros_like(cg).scatter_(1, keep_indices, 1).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
