{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ami\t\t  model_configs     tedlium\n",
      "checkpoints_done  roberta_base.bin  Untitled-1.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, numpy as np, torch.nn as nn, torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-10-18 10:38:47 optimizers:55] Apex was not found. Using the lamb or fused_adam optimizer will error out.\n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Union, Dict, Any, Tuple, Callable\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.asr.parts.submodules.multi_head_attention import RelPositionMultiHeadAttention\n",
    "from nemo.collections.asr.parts.utils.helpers import (\n",
    "    exists,\n",
    "    isfalse\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.intermediate_act_fn = F.gelu\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_head,\n",
    "        n_feat,\n",
    "        dropout_rate,\n",
    "        pos_bias_u=None,\n",
    "        pos_bias_v=None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self = RelPositionMultiHeadAttention(\n",
    "            n_head,\n",
    "            n_feat,\n",
    "            dropout_rate,\n",
    "            pos_bias_u=pos_bias_u,\n",
    "            pos_bias_v=pos_bias_v\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.LayerNorm = nn.LayerNorm(n_feat)\n",
    "\n",
    "    def forward(self, x, mask, return_attentions=False):\n",
    "        attn_out = self.self(query=x, key=x, value=x, mask=mask, return_attentions=return_attentions)\n",
    "        attention_out, maps = attn_out if return_attentions else (attn_out, None)\n",
    "        x = self.dropout(attention_out) + x\n",
    "        x = self.LayerNorm(x)\n",
    "        return (x, maps) if return_attentions else x\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_size: int,\n",
    "            num_attention_heads: int,\n",
    "            dropout_rate_attn: float,\n",
    "            pos_bias_u,\n",
    "            pos_bias_v,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = BertAttention(\n",
    "            n_head=num_attention_heads,\n",
    "            n_feat=hidden_size,\n",
    "            dropout_rate=dropout_rate_attn,\n",
    "            pos_bias_u=pos_bias_u,\n",
    "            pos_bias_v=pos_bias_v\n",
    "        )\n",
    "       \n",
    "        self.intermediate = BertIntermediate(\n",
    "            hidden_size=hidden_size,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "        )\n",
    "        \n",
    "        self.output = BertOutput(\n",
    "            hidden_size=hidden_size,\n",
    "            intermediate_size=hidden_size * 4,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[torch.Tensor]:\n",
    "\n",
    "        self_attention_outputs = self.attention(\n",
    "            query=hidden_states,\n",
    "            key=hidden_states,\n",
    "            value=hidden_states,\n",
    "            mask=attention_mask,\n",
    "            return_attentions=output_attentions,\n",
    "        )\n",
    "        attention_output, output = self_attention_outputs if output_attentions else (self_attention_outputs, None)\n",
    "        layer_output = self.feed_forward_chunk(attention_output)\n",
    "\n",
    "        outputs = (layer_output,) + outputs\n",
    "        return outputs\n",
    "\n",
    "    def freeze_params(self, freeze: bool = True):\n",
    "        for p in self.parameters():\n",
    "            p.requires_grad = not freeze\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoderCTC(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_encoder_layers: int,\n",
    "        hidden_size: int,\n",
    "        num_attention_heads: int,\n",
    "        dropout_rate_attn: float,\n",
    "        ctc_vocab_size: int,\n",
    "        acoustic_hidden_size: int,\n",
    "        pos_bias_u,\n",
    "        pos_bias_v,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.dropout_rate_attn = dropout_rate_attn\n",
    "        self.ctc_vocab_size = ctc_vocab_size\n",
    "        self.acoustic_hidden_size = acoustic_hidden_size\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(num_encoder_layers):\n",
    "            layer = BertLayer(\n",
    "                hidden_size=hidden_size,\n",
    "                num_attention_heads=num_attention_heads,\n",
    "                dropout_rate_attn=dropout_rate_attn,\n",
    "                pos_bias_u=pos_bias_u,\n",
    "                pos_bias_v=pos_bias_v,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        self.fuse_inputs = nn.Linear(hidden_size + ctc_vocab_size, hidden_size)\n",
    "    \n",
    "        self.project_to_acoustic = nn.Linear(hidden_size, acoustic_hidden_size)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        layer_num: int,\n",
    "        residual: torch.Tensor = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    ):\n",
    "        assert layer_num >= 0 and layer_num < len(self.layers), f\"layer_num must be between 0 and {len(self.layers)}\"\n",
    "        layer = self.layers[layer_num]\n",
    "        b, n, c = x.shape\n",
    "        residual = residual if exists(residual) else torch.zeros(b, n, self.hidden_size, device=x.device)\n",
    "        x = torch.cat((x, residual), dim=-1) # concat along feature dim\n",
    "        x = self.fuse_inputs(x)\n",
    "\n",
    "        x = layer(x, attention_mask=attention_mask)\n",
    "        \n",
    "        new_residual = x\n",
    "        x = self.project_to_acoustic(x)\n",
    "        return x, new_residual\n",
    "\n",
    "    @staticmethod\n",
    "    def model_surgery(roberta_state_dict):\n",
    "        def tx_transforms(tx:str):\n",
    "            tx = tx.replace(\"roberta.encoder.layer.\", \"layers.\")\n",
    "            tx = tx.replace('.self.query.', '.self.linear_q.')\n",
    "            tx = tx.replace('.self.key.', '.self.linear_k.')\n",
    "            tx = tx.replace('.self.value.', '.self.linear_v.')\n",
    "            tx = tx.replace('.attention.output.dense.', '.attention.self.linear_out.')\n",
    "            tx = tx.replace('.attention.output.LayerNorm.', '.attention.LayerNorm.')\n",
    "            return tx\n",
    "        '''\n",
    "        Changes the name of robertas keys to match my model\n",
    "        '''\n",
    "        new_state_dict = {}\n",
    "        for k, v in roberta_state_dict.items():\n",
    "            new_k = tx_transforms(k)\n",
    "            new_state_dict[new_k] = v\n",
    "        return new_state_dict\n",
    "\n",
    "    def load_roberta(self, roberta_path, map_location='cpu'):\n",
    "        roberta_state_dict = torch.load(roberta_path, map_location=map_location)\n",
    "        roberta_state_dict = self.model_surgery(roberta_state_dict)\n",
    "        self.load_state_dict(roberta_state_dict, strict=False)\n",
    "\n",
    "    def freeze_BERT(self, freeze: bool = True):\n",
    "        '''\n",
    "        Use this for freezing the BERT layers, but not the other FF layers\n",
    "        '''\n",
    "        for layer in self.layers:\n",
    "            layer.freeze_params(freeze=freeze)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT = BertEncoderCTC(\n",
    "    num_encoder_layers=12, \n",
    "    hidden_size=768, \n",
    "    num_attention_heads=12, \n",
    "    dropout_rate_attn=0.1, \n",
    "    pos_bias_u=None, \n",
    "    pos_bias_v=None,\n",
    "    ctc_vocab_size=128,\n",
    "    acoustic_hidden_size=256,   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_surgery(roberta_state_dict):\n",
    "    def tx_transforms(tx:str):\n",
    "        tx = tx.replace(\"roberta.encoder.layer.\", \"layers.\")\n",
    "        tx = tx.replace('.self.query.', '.self.linear_q.')\n",
    "        tx = tx.replace('.self.key.', '.self.linear_k.')\n",
    "        tx = tx.replace('.self.value.', '.self.linear_v.')\n",
    "        tx = tx.replace('.attention.output.dense.', '.attention.self.linear_out.')\n",
    "        tx = tx.replace('.attention.output.LayerNorm.', '.attention.LayerNorm.')\n",
    "        return tx\n",
    "    '''\n",
    "    Changes the name of robertas keys to match my model\n",
    "    '''\n",
    "    new_state_dict = {}\n",
    "    for k, v in roberta_state_dict.items():\n",
    "        new_k = tx_transforms(k)\n",
    "        new_state_dict[new_k] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.attention.self.pos_bias_u', 'layers.0.attention.self.pos_bias_v', 'layers.0.attention.self.linear_pos.weight', 'layers.1.attention.self.pos_bias_u', 'layers.1.attention.self.pos_bias_v', 'layers.1.attention.self.linear_pos.weight', 'layers.2.attention.self.pos_bias_u', 'layers.2.attention.self.pos_bias_v', 'layers.2.attention.self.linear_pos.weight', 'layers.3.attention.self.pos_bias_u', 'layers.3.attention.self.pos_bias_v', 'layers.3.attention.self.linear_pos.weight', 'layers.4.attention.self.pos_bias_u', 'layers.4.attention.self.pos_bias_v', 'layers.4.attention.self.linear_pos.weight', 'layers.5.attention.self.pos_bias_u', 'layers.5.attention.self.pos_bias_v', 'layers.5.attention.self.linear_pos.weight', 'layers.6.attention.self.pos_bias_u', 'layers.6.attention.self.pos_bias_v', 'layers.6.attention.self.linear_pos.weight', 'layers.7.attention.self.pos_bias_u', 'layers.7.attention.self.pos_bias_v', 'layers.7.attention.self.linear_pos.weight', 'layers.8.attention.self.pos_bias_u', 'layers.8.attention.self.pos_bias_v', 'layers.8.attention.self.linear_pos.weight', 'layers.9.attention.self.pos_bias_u', 'layers.9.attention.self.pos_bias_v', 'layers.9.attention.self.linear_pos.weight', 'layers.10.attention.self.pos_bias_u', 'layers.10.attention.self.pos_bias_v', 'layers.10.attention.self.linear_pos.weight', 'layers.11.attention.self.pos_bias_u', 'layers.11.attention.self.pos_bias_v', 'layers.11.attention.self.linear_pos.weight', 'fuse_inputs.weight', 'fuse_inputs.bias', 'project_to_acoustic.weight', 'project_to_acoustic.bias'], unexpected_keys=['roberta.embeddings.word_embeddings.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BERT.load_state_dict(model_surgery(torch.load('roberta_base.bin', map_location='cpu')), strict=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('k2_custom-nemo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c94c8ffa67fdebd9384b5746b8c4850bc2cec88ff489992126dcd0aca228c275"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
